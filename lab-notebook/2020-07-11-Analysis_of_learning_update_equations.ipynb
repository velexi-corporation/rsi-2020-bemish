{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020-07-11: Analysis of Learning Update Equations\n",
    "-------------------------------------------------\n",
    "\n",
    "*Author*: Kevin Chu `<kevin@velexi.com>`\n",
    "\n",
    "*Last Updated*: 2020-07-12\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "### Context\n",
    "\n",
    "In his computational experiments, Luke has observed that convergence of node\n",
    "threshold values is sensitive to the choice of the bit probability $p_i$ set\n",
    "for each node. This observation prompted a deeper mathematical analysis of the\n",
    "update algorithms.\n",
    "\n",
    "### 1. Potential problems with the Foldiak algorithm learning rules\n",
    "\n",
    "#### The theshold update equation converges only when $E[y_i] \\rightarrow p_i$\n",
    "\n",
    "Let $t_i^s$ denote the threshold for the $i$-th node after $s$ learning cycles.\n",
    "Noting that $\\Delta t_i^s = t_i^s - t_i^{s-1}$, the Foldiak's threshold update\n",
    "equation may be expressed as\n",
    "\n",
    "$$\n",
    "  \\Delta t_i^s = t_i^s - t_i^{s-1}\n",
    "  = \\gamma (y_i^s (\\mathbf{Q}, \\mathbf{W}, \\mathbf{t}) - p_i),\n",
    "$$\n",
    "\n",
    "where the dependence of $y_i^s$ on the learned memories, inhibition weights,\n",
    "the node thresholds, and learning cycle index have been made explicit.\n",
    "\n",
    "Summing over $s$, we can derive the following expression for $t_i^s$:\n",
    "\n",
    "$$\n",
    "  t_i^s - t_i^0\n",
    "  = \\sum_{\\tau = 1}^s \\Delta t_i^\\tau\n",
    "  = \\gamma \\sum_{\\tau = 1}^s (y_i^\\tau- p_i)\n",
    "  = \\gamma s \\left[ \\left(\n",
    "                      \\frac{1}{s} \\sum_{\\tau = 1}^s y_i^\\tau\n",
    "                    \\right)\n",
    "                  - p_i\n",
    "             \\right].\n",
    "$$\n",
    "\n",
    "Note that if the expected value of $y_i$ exists, the sum approaches $E[y_i]$.\n",
    "\n",
    "___Relevant Cases___\n",
    "\n",
    "* $E[y_i] = p_i$. In this case, the limit on the right-hand side is\n",
    "  indeterminate, so it can approach any value. Computational experiments\n",
    "  indicate that $t_i^s$ approaches a constant when $p_i$ is set to $E[y_i]$,\n",
    "  which suggests that\n",
    "\n",
    "  $$\n",
    "    E \\left[ \\frac{1}{s} \\sum_{\\tau = 1}^s y_i^{\\tau} \\right] - p_i\n",
    "    \\approx \\frac{\\alpha}{s} + \\textrm{(higher-order terms)}.\n",
    "  $$\n",
    "\n",
    "  __Note__: it might be possible to prove this relation and determine $\\alpha$\n",
    "  from theoretical convergence rates for the law of large numbers, but I don't\n",
    "  know the convergence rate results of the top of my head (and didn't find\n",
    "  them from a quick internet search).\n",
    "\n",
    "* $E[y_i] \\ne p_i$. The right-hand side tends towards\n",
    "\n",
    "  $$\n",
    "    \\gamma (E[y_i] - p_i) s,\n",
    "  $$\n",
    "\n",
    "  so the threshold value is expected to tend towards a linear function of $s$.\n",
    "\n",
    "___Observations___\n",
    "\n",
    "* The computational experiments suggest that the key error approaches a\n",
    "  constant value as $s \\rightarrow \\infty$. Therefore, it seems reasonable to\n",
    "  assume that $y_i$ has a finite expected value.\n",
    "\n",
    "* Note that the contribution of the initial threshold value does not\n",
    "  automatically vanish as $s \\rightarrow \\infty$. We need to rely on the\n",
    "  expectation value of $y_i$ to be equal to $p_i$.\n",
    "\n",
    "#### The inhibition weight and threshold update equation behave similarly\n",
    "\n",
    "* An analysis of the inhibition equation using the same approach as for the\n",
    "  threshold equation leads to similar conclusions for the inhibition weights.\n",
    "\n",
    "* The conditions required for convergence of the inhibition weights are:\n",
    "\n",
    "  * $E[y_i] = p_i$ for all $i$\n",
    "\n",
    "  * $Cov(y_i, y_j) = 0$ for all $i$, $j$ so that\n",
    "\n",
    "    $$\n",
    "      \\frac{1}{s} \\sum_{\\tau = 1}^s E[y_i y_j] - p_i p_j \\rightarrow 0\n",
    "    $$\n",
    "\n",
    "### 2. Digression: notes on first-order differential and difference equations\n",
    "\n",
    "#### First-order, linear, constant coefficient differential equations\n",
    "\n",
    "A first-order, linear, constant coefficient differential equation is an\n",
    "equation of the form\n",
    "\n",
    "$\n",
    "  a_1 y' + a_0 y = f(t)\n",
    "$\n",
    "\n",
    "where $a_i$ are constants independent of $t$ and $f(t)$ is an inhomogenous\n",
    "term (e.g., a \"forcing function\"). To solve this equation, we first find the\n",
    "solution to the homogeneous equation\n",
    "\n",
    "$\n",
    "  a_1 y' + a_0 y = 0\n",
    "$\n",
    "\n",
    "and then use that solution as an integrating factor.\n",
    "\n",
    "The solution to the homogeneous equation can be found by using an ansatz\n",
    "solution of the form $y(t) = C e^{r t}$ where $C$ and $r$ are constants.\n",
    "Plugging this into the differential equation yields a linear equation in $r$\n",
    "(for higher-order differential equations, the same ansatz solution leads to\n",
    "a polynomial equation in $r$):\n",
    "\n",
    "$\n",
    "  a_1 r + a_0 = 0.\n",
    "$\n",
    "\n",
    "Solving for $r$, we find that $r = -a_0 / a_1$ and that the solution to the\n",
    "homogeneous equation is\n",
    "\n",
    "$\n",
    "  C e^{r t} = C e^{-\\left(\\frac{a_0}{a_1}\\right) t}\n",
    "$\n",
    "\n",
    "To solve the inhomogeneous equation, we observe that\n",
    "\n",
    "$\n",
    "  \\frac{d}{dt} \\left( e^{-rt} y \\right)\n",
    "  = e^{-rt} (y' -r y)\n",
    "  = \\left( \\frac{1}{a_1} \\right) e^{-rt} (a_1 y' - a_1 r y)\n",
    "  = \\left( \\frac{1}{a_1} \\right) e^{-rt} (a_1 y' + a_0 y)\n",
    "$\n",
    "\n",
    "Therefore, we can transform the inhomogeneous equation into simple integration\n",
    "problem by multiplying the equation by the _integrating factor_ $e^{-rt}$:\n",
    "\n",
    "$\n",
    "  e^{-rt} (a_1 y' + a_0 y)\n",
    "  = a_1 \\frac{d}{dt} \\left( e^{-rt} y \\right)\n",
    "  = e^{-rt} f(t)\n",
    "$\n",
    "\n",
    "Integrating and multiplying through by $e^{rt} / a_1$, we obtain the solution\n",
    "\n",
    "$\n",
    "  y(t) - y(0)\n",
    "  = \\frac{e^{rt}}{a_1} \\int_0^t e^{-rs} f(s) ds\n",
    "  = \\frac{1}{a_1} \\int_0^t e^{r(t - s)} f(s) ds\n",
    "  = \\frac{1}{a_1} \\int_0^t e^{rs} f(t-s) ds ds\n",
    "$\n",
    "\n",
    "#### First-order, linear, constant coefficient difference equations\n",
    "\n",
    "A first-order, linear, constant coefficient difference (or recurrence) equation\n",
    "is an equation of the form\n",
    "\n",
    "$\n",
    "  a_1 y_{n+1} + a_0 y_{n} = f_n\n",
    "$\n",
    "\n",
    "where $a_i$ are constants independent of $n$ and $f_n$ is an inhomogeneous\n",
    "term. To draw the connection to differential equations, we define the\n",
    "_difference operator_ $\\Delta$ which can be defined by\n",
    "\n",
    "$\n",
    "  \\Delta y_{n} = y_{n} - y_{n-1}.\n",
    "$\n",
    "\n",
    "Note that, unlike the differential operator, there are multiple conventions\n",
    "that could be used to define $\\Delta$. For instance, we could also define\n",
    "$\\Delta$ using a forward difference instead of a backward difference. The\n",
    "different conventions yield slightly different equations (mainly in the way\n",
    "that the coefficients are combined and with respect to indexing), but all\n",
    "consistent conventions should lead to the same solutions.\n",
    "\n",
    "Written in terms of the difference operator $\\Delta$, the above recurrence\n",
    "relation can be expressed as\n",
    "\n",
    "$\n",
    "  a_1 \\Delta y_{n+1} + (a_0 + a_1) y_{n} = f_n\n",
    "$\n",
    "\n",
    "Defining new coefficients $b_0 = a_0 + a_1$ and $b_1 = a_1$, we arrive at the\n",
    "discrete analog of the first-order, linear, constant coefficient differential\n",
    "equation:\n",
    "\n",
    "$\n",
    "  b_1 \\Delta y_{n+1} + b_0 y_{n} = f_n.\n",
    "$\n",
    "\n",
    "We can solve this equation using an approach analogous to the solution of the\n",
    "differential equation, but we have to be a little more careful with keeping\n",
    "track of indices.\n",
    "\n",
    "To solve the homogeneous equation, we look for solutions of the form $C r^n$\n",
    "and work with the original recurrence equation. Note that (1) looking for\n",
    "solutions of the form $C e^{s n}$ and (2) using the form of the equation\n",
    "involving the $\\Delta$ operator yields completely equivalent results. It's just\n",
    "easier and less tedious to work with the original recurrence equation and the\n",
    "simpler ansatz. Plugging in our ansatz solution, we find that\n",
    "\n",
    "$\n",
    "  a_1 r^{n+1} + a_0 r^{n} = 0,\n",
    "$\n",
    "\n",
    "which implies that\n",
    "\n",
    "$\n",
    "  a_1 r + a_0 = 0.\n",
    "$\n",
    "\n",
    "Therefore, the solution to the homogeneous equation is\n",
    "\n",
    "$\n",
    "  C \\left(- \\frac{a_0}{a_1}\\right)^n\n",
    "$\n",
    "\n",
    "To solve the inhomogeneous difference equation, it is convenient to use the\n",
    "form of the equation involve the difference operator. Like the differential\n",
    "equation case, we observe that we can multiply the left-hand side of the\n",
    "equation by an appropriate _summation factor_ to transform the equation into\n",
    "a simple summation problem.\n",
    "\n",
    "Observe that\n",
    "\n",
    "$\n",
    "  \\Delta \\left( r^{-(n+1)} y_{n+1} \\right)\n",
    "  = r^{-(n+1)} y_{n+1} - r^{-n} y_{n} \\\\\n",
    "  =   r^{-(n+1)} y_{n+1}\n",
    "    + (-r^{-(n+1)} y_{n} + r^{-(n+1)} y_{n})\n",
    "    - r^{-n} y_{n} \\\\\n",
    "  =   ( r^{-(n+1)} y_{n+1} - r^{-(n+1)} y_{n} )\n",
    "    + r^{-(n+1)} y_{n} - r^{-n} y_{n} \\\\\n",
    "  =   r^{-(n+1)} \\Delta y_{n+1} + r^{-(n+1)} y_{n} - r^{-n} y_{n} \\\\\n",
    "  =   r^{-(n+1)} \\left( \\Delta y_{n+1} + (1 - r) y_{n} \\right ) \\\\\n",
    "  =   r^{-(n+1)} \\left( \\frac{b_1 \\Delta y_{n+1} + b_0 y_{n}}{b_1} \\right )\n",
    "$\n",
    "\n",
    "Multiplying the $\\Delta$-form of the difference equation by the summation\n",
    "factor, we transform the inhomogeneous difference equation into\n",
    "\n",
    "$\n",
    "  r^{-(n+1)} \\left( b_1 \\Delta y_{n+1} + b_0 y_{n} \\right )\n",
    "  = b_1 \\Delta \\left( r^{-(n+1)} y_{n+1} \\right)\n",
    "  = r^{-(n+1)} f_n\n",
    "$\n",
    "\n",
    "Summing and multiplying the result by $r^n / b_1$, we obtain the solution\n",
    "\n",
    "$\n",
    "  y_n\n",
    "  =   r^n y_0\n",
    "    + \\frac{r^n}{b_1} \\sum_{i = 0}^{n-1} r^{-(i+1)} f_i\n",
    "  =   r^n y_0\n",
    "    + \\frac{1}{b_1} \\sum_{i = 0}^{n-1} r^{n-i-1} f_i\n",
    "  =   r^n y_0\n",
    "    + \\frac{1}{b_1} \\sum_{i = 0}^{n-1} r^{i} f_{n-i-1}\n",
    "$\n",
    "\n",
    "#### Comments about differential/difference equations and their solutions\n",
    "\n",
    "* __Long-time behavior of solutions of the homogeneous equation__\n",
    "\n",
    "  * For the differential equation, when $a_1$ and $a_0$ have opposite sign, the\n",
    "    solution of the homogeneous equation has an exponentially decaying\n",
    "    solution.  This implies that the initial condition decay towards zero at\n",
    "    long times.\n",
    "\n",
    "  * For the difference equation, when $a_1$ and $a_0$ have opposite sign and\n",
    "    $|a_0| < |a_1|$, the solution of the homogeneous equation has an\n",
    "    exponentially decaying solution. This implies that the initial condition\n",
    "    decay towards zero at long times.\n",
    "\n",
    "* __Relation of inhomogenous solution to exponential moving averages__\n",
    "\n",
    "  * The inhomogeneous part of the solution to an inhomogeneous difference\n",
    "    equation has the form of an exponential moving average (exponential\n",
    "    smoothing) of the inhomogeneous term. In the long-time limit, the\n",
    "    exponential moving average approaches the expected value of the\n",
    "    inhomogeneous term.\n",
    "\n",
    "  * For differential equations, the inhomogeneous part of the solution to an\n",
    "    inhomogeneous equation is the continuous analogue of an exponentially\n",
    "    moving average.\n",
    "\n",
    "* __Relation of equations to closed-loop (feedback) systems__\n",
    "\n",
    "  * The dependence of the derivative (difference) on the current (recent)\n",
    "    values of the solution indicates the presence of a feedback loop in the\n",
    "    system modeled by the equations. It is this feedback that guarantees that\n",
    "    transients will decay (for appropriate values of the coefficients).\n",
    "\n",
    "    In general, closed-loop systems are more stable and operate more reliably\n",
    "    than open-loop systems (i.e., no feedback).\n",
    "\n",
    "### 3. Exercises / Open Questions\n",
    "\n",
    "* Using an understanding of the behavior of first-order, linear, constant\n",
    "  coefficient differential and difference equations, what are possible\n",
    "  modifications that could be made to the Foldiak algorithm to improve its\n",
    "  stability and/or reduce its sensitivity to parameters choices?\n",
    "\n",
    "* Could Foldiak's algorithm be modified so that the bit probabilities arise\n",
    "  naturally without having to be specified?\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
